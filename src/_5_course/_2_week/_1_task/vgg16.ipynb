{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: Анализ изображений\n",
    "\n",
    "Задача заключается в том, чтобы применить предобученную на imagenet нейронную сеть на практической задаче классификации автомобилей. \n",
    "\n",
    "Учиться применять нейронные сети для анализа изображений мы будем на библиотеке TensorFlow. Это известный опенсорсный проект, разработанный инженерами Google Brain Team. Подробнее почитать о TensorFlow можно на официальном сайте, на [гитхабе](https://github.com/tensorflow/tensorflow) или [на хабре](https://habrahabr.ru/post/270543/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка окружения\n",
    "\n",
    "В первую очередь нам будет необходимо установить **TensorFlow**.\n",
    "* [Инструкции по установке на сайте](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup).\n",
    "* Если есть опыт работы с Docker, то можно воспользоваться готовым [докер-контейнером с тензорфлоу](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#docker-installation).\n",
    "\n",
    "**Важно!** Если вы пользователь Windows, то уставить tensorflow напрямую, к сожалению, не получится:\n",
    "\n",
    "* для пользователей Windows 10 (и выше) нужно использовать **Docker** ([ссылка на дистрибутив](https://www.docker.com/products/docker#/windows));\n",
    "* если у вас Windows старше версии 10, то и вариант с докером не подойдет — он не установится. В таком случае советуем установить линукс на локальную машину как еще одну операционную систему (поможет избежать страданий в будущем при работе с некоторыми библиотеками для ML).\n",
    "\n",
    "Если же поставить Tensorflow на вашу машину никак не получается, мы предлагаем воспользоваться одним из облачных сервисов, в который необходимо установить линукс-образ. Самые популярные облачные сервисы [AWS](https://aws.amazon.com) и [DigitalOcean](http://digitalocean.com) предоставляют бесплатные инстансы (имейте в виду, что для того, чтобы ими воспользоваться, нужно будет привязать кредитную карту).\n",
    "\n",
    "Чтобы освоить компьютерное зрение (или другие интересные задачи из области ML и AI), так или иначе придётся научиться работать с библиотеками нейронных сетей, линуксом и виртуальными серверами. Например, для более масштабных практических задач, крайне необходимы сервера с GPU, а с ними уже локально работать не получиться.\n",
    "\n",
    "Тем не менее, мы понимаем, что в силу временных ограничений курса кто-то может успеть установить TensorFlow. Поэтому мы сделали пункты 1 и 2 необязательными. На оценку они не повлияют — можете сразу переходить к третьему пункту.\n",
    "\n",
    "Помимо tensorflow, потребуется библиотека **`scipy`**. Если вы уже работали с Anaconda и/или выполняли задания в нашей специализации, то она должна присутствовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Скачать данные нужно тут: https://yadi.sk/d/6m_KbM4HvmLfs \n",
    "\n",
    "Данные это часть выборки _Cars Dataset_ ([link](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)). Исходный датасет содержит 16,185 изображений автомобилей, принадлежащих к 196 классам. Данные разделены на 8,144 тренировочных и 8,041 тестовых изображений, при этом каждый класс разделён приблизительно поровну между тестом и трейном. Все классы уровня параметров _Марка_, _Год_, _Модель_ и др. (например, _2012 Tesla Model S or 2012 BMW M3 coupe_).\n",
    "\n",
    "В нашем же случае в `train` 204 изображения, и в `test` — 202 изображения.\n",
    "\n",
    "## Что делать\n",
    "\n",
    "Помимо данных, потребуется скачать:\n",
    "\n",
    "* [код](https://github.com/ton4eg/coursera_pa), \n",
    "* веса модели [по ссылке](https://yadi.sk/d/9-3kXyxRvnBwh)\n",
    "\n",
    "Положите данные, код и модель в одну папку. У вас должна получиться такая структура:\n",
    "\n",
    "```\n",
    "/assignment-computer-vision/\n",
    "|\n",
    "|-- test              # папки  \n",
    "|    `---- ...        # с\n",
    "|-- train             # картинками\n",
    "|    `---- ...\n",
    "|\n",
    "|-- class_names.txt   # имена классов, номер строки соответствует id класса\n",
    "|-- results.txt       # соответствие имя картинки — id класса\n",
    "|-- vgg16_weights.npz # веса модели в формате tensorflow\n",
    "|\n",
    "|-- vgg16.py            # основной скрипт\n",
    "|-- imagenet_classes.py \n",
    "|\n",
    "`-- beach.jpg         # картиночка с пляжем\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск из Docker gcr.io/tensorflow/tensorflow\n",
    "* докер запускает Jupyter Notebook с рабочей папкой /notebooks\n",
    "* в докере не хватает библиотек sklearn и Pillow\n",
    "\n",
    "Рекомендуется запускать докер командой\n",
    "\n",
    "docker run -it -p 127.0.0.1:8888:8888 -v $PWD:/notebooks gcr.io/tensorflow/tensorflow\n",
    "\n",
    "при запуске из папки, где лежит данный ноутбук и все нужные файлы, они сразу окажутся в рабочей папке Jupyter \n",
    "Notebook.\n",
    "\n",
    "Следующие две ячейки содержат команды, доустанавливающие нужные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\public\\pythonprojects\\anaconda3\\lib\\site-packages (from sklearn)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn: started\n",
      "  Running setup.py bdist_wheel for sklearn: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\C040~1\\AppData\\Local\\pip\\Cache\\wheels\\d7\\db\\a3\\1b8041ab0be63b5c96c503df8e757cf205c2848cf9ef55f85e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading Pillow-4.2.1-cp36-cp36m-win_amd64.whl (1.5MB)\n",
      "Collecting olefile (from pillow)\n",
      "  Downloading olefile-0.44.zip (74kB)\n",
      "Building wheels for collected packages: olefile\n",
      "  Running setup.py bdist_wheel for olefile: started\n",
      "  Running setup.py bdist_wheel for olefile: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\C040~1\\AppData\\Local\\pip\\Cache\\wheels\\20\\58\\49\\cc7bd00345397059149a10b0259ef38b867935ea2ecff99a9b\n",
      "Successfully built olefile\n",
      "Installing collected packages: olefile, pillow\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 381, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\C040~1\\\\AppData\\\\Local\\\\Temp\\\\pip-s4ch4sqp-uninstall\\\\users\\\\public\\\\pythonprojects\\\\anaconda3\\\\lib\\\\site-packages\\\\pil\\\\_imaging.cp36-win_amd64.pyd'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 795, in install\n",
      "    requirement.commit_uninstall()\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 767, in commit_uninstall\n",
      "    self.uninstalled.commit()\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\req\\req_uninstall.py\", line 142, in commit\n",
      "    rmtree(self.save_dir)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 49, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 212, in call\n",
      "    raise attempt.get()\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 247, in get\n",
      "    six.reraise(self.value[0], self.value[1], self.value[2])\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\six.py\", line 686, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 200, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\utils\\__init__.py\", line 102, in rmtree\n",
      "    onerror=rmtree_errorhandler)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 488, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 378, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 378, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 378, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  [Previous line repeated 3 more times]\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\shutil.py\", line 383, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Users\\Public\\PythonProjects\\Anaconda3\\lib\\site-packages\\pip\\utils\\__init__.py\", line 114, in rmtree_errorhandler\n",
      "    func(path)\n",
      "PermissionError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\C040~1\\\\AppData\\\\Local\\\\Temp\\\\pip-s4ch4sqp-uninstall\\\\users\\\\public\\\\pythonprojects\\\\anaconda3\\\\lib\\\\site-packages\\\\pil\\\\_imaging.cp36-win_amd64.pyd'\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем всё, что нам нужно для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspired by\n",
    "# http://www.cs.toronto.edu/~frossard/post/vgg16/                               \n",
    "# Model from https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md     #\n",
    "# Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow      #\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "import sys\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом классе содержится описание модели VGG - структура, инициализация, загрузка весов. Следует помнить - пока не запущена сессия Tensorflow, никакой реальной работы не производится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция сохранения в файл ответа, состоящего из одного числа\n",
    "def save_answerNum(fname,number):\n",
    "    with open(fname,\"w\") as fout:\n",
    "        fout.write(str(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция сохранения в файл ответа, представленного массивом\n",
    "def save_answerArray(fname,array):\n",
    "    with open(fname,\"w\") as fout:\n",
    "        fout.write(\"\\n\".join([str(el) for el in array]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загрузка словаря из текстового файла. Словарь у нас используется для сохранения меток классов в выборке data.\n",
    "def load_txt(fname):\n",
    "    line_dict = {}\n",
    "    for line in open(fname):\n",
    "        fname, class_id = line.strip().split()\n",
    "        line_dict[fname] = class_id\n",
    "\n",
    "    return line_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция обработки отдельного изображения, печатает метки TOP-5 классов и уверенность модели в каждом из них.\n",
    "def process_image(fname):\n",
    "    img1 = imread(fname, mode='RGB')\n",
    "    img1 = imresize(img1, (224, 224))\n",
    "    \n",
    "    prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    preds = (np.argsort(prob)[::-1])[0:5]\n",
    "    for p in preds:\n",
    "        print(class_names[p], prob[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Инициируем TF сессию, и инициализируем модель. На этом шаге модель загружает веса. Веса - это 500Мб в сжатом виде\n",
    "# и ~2.5Гб в памяти, процесс их загрузки послойно выводится ниже этой ячейки, и если вы увидите этот вывод ещё раз - \n",
    "# у вас неистово кончается память. Остановитесь. Также, не запускайте эту ячейку на выполнение больше одного раза\n",
    "# за запуск ядра Jupyter.\n",
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg = vgg16(imgs, 'data/vgg16_weights.npz', sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все ячейки выше не нуждаются в модификации для выполнения задания, и необходимы к исполнению только один раз, в порядке следования. Повторный запуск ячейки с инициализацией модели будет сжирать память. Вы предупреждены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\\. \n",
    "\n",
    "Для начала нужно запустить готовую модель `vgg16`, предобученную на `imagenet`. Модель обучена с помощью `caffe` и сконвертирована в формат `tensorflow` - `vgg16_weights.npz`. Скрипт, иллюстрирующий применение этой модели к изображению, возвращает топ-5 классов из `imagenet` и уверенность в этих классах.\n",
    "\n",
    "**Задание:** Загрузите уверенность для первого класса для изображения `train/00002.jpg` с точностью до 1 знака после запятой в файл с ответом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minivan 0.400031\n",
      "beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon 0.117361\n",
      "sports car, sport car 0.0500518\n",
      "car wheel 0.0453139\n",
      "grille, radiator grille 0.0320824\n"
     ]
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "process_image(\"data/train/00002.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_answerNum(\"vgg16_answer1.txt\", round(0.400031, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\\. \n",
    "\n",
    "Научитесь извлекать `fc2` слой. Возьмите за основу код `process_image`, и модифицируйте, чтобы вместо последнего слоя извлекались выходы `fc2`.\n",
    "\n",
    "**Задание:** Посчитайте `fc2` для картинки `train/00002.jpg`.  Запишите первые 20 компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1 = imread('data/train/00002.jpg', mode='RGB')\n",
    "img1 = imresize(img1, (224, 224))\n",
    "prob = sess.run(vgg.fc2, feed_dict={vgg.imgs: [img1]})[0]\n",
    "\n",
    "save_answerArray(\"vgg16_answer2.txt\", prob[:20] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\\. \n",
    "\n",
    "Теперь необходимо дообучить классификатор на нашей базе. В качестве бейзлайна предлагается воспользоваться классификатором `svm` из пакета `scipy`.\n",
    "\n",
    "- Модифицировать функцию `get_features` и добавить возможность вычислять `fc2`. (Аналогично второму заданию).\n",
    "- Применить `get_feautures`, чтобы получить `X_test` и `Y_test`.\n",
    "- Воспользоваться классификатором `SVC` с `random_state=0`.\n",
    "\n",
    "> **Важно!** Если вам не удалось поставить `tensorflow`, то необходимо вместо использования функции `get_features`, загрузить предпосчитанные `X`, `Y`, `X_test`, `Y_test` из архива: https://yadi.sk/d/RzMOK8Fjvs6Ln и воспользоваться функцией `np.load` для их загрузки, а после этого два последних пункта.\n",
    "\n",
    "**Задание:** Сколько правильных ответов получается на валидационной выборке из папки `test`? Запишите в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция, возвращающая признаковое описание для каждого файла jpg в заданной папке\n",
    "def get_features(folder, ydict):\n",
    "    \n",
    "    paths = glob.glob(folder)\n",
    "    X = np.zeros((len(paths), 4096))\n",
    "    Y = np.zeros(len(paths))\n",
    "\n",
    "    for i,img_name in enumerate(paths):\n",
    "        print(img_name)\n",
    "        base = os.path.basename(img_name)\n",
    "        Y[i] = ydict[base]\n",
    "\n",
    "        img1 = imread(img_name, mode='RGB')\n",
    "        img1 = imresize(img1, (224, 224))\n",
    "        # Здесь ваш код. Нужно получить слой fc2\n",
    "        fc2 = sess.run(vgg.fc2, feed_dict={vgg.imgs: [img1]})[0]\n",
    "        \n",
    "        X[i, :] = fc2\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train\\00002.jpg\n",
      "./data/train\\00065.jpg\n",
      "./data/train\\00069.jpg\n",
      "./data/train\\00081.jpg\n",
      "./data/train\\00150.jpg\n",
      "./data/train\\00151.jpg\n",
      "./data/train\\00163.jpg\n",
      "./data/train\\00198.jpg\n",
      "./data/train\\00208.jpg\n",
      "./data/train\\00221.jpg\n",
      "./data/train\\00236.jpg\n",
      "./data/train\\00244.jpg\n",
      "./data/train\\00255.jpg\n",
      "./data/train\\00292.jpg\n",
      "./data/train\\00308.jpg\n",
      "./data/train\\00320.jpg\n",
      "./data/train\\00340.jpg\n",
      "./data/train\\00370.jpg\n",
      "./data/train\\00374.jpg\n",
      "./data/train\\00392.jpg\n",
      "./data/train\\00405.jpg\n",
      "./data/train\\00410.jpg\n",
      "./data/train\\00424.jpg\n",
      "./data/train\\00425.jpg\n",
      "./data/train\\00462.jpg\n",
      "./data/train\\00503.jpg\n",
      "./data/train\\00522.jpg\n",
      "./data/train\\00631.jpg\n",
      "./data/train\\00634.jpg\n",
      "./data/train\\00635.jpg\n",
      "./data/train\\00670.jpg\n",
      "./data/train\\00691.jpg\n",
      "./data/train\\00697.jpg\n",
      "./data/train\\00700.jpg\n",
      "./data/train\\00707.jpg\n",
      "./data/train\\00740.jpg\n",
      "./data/train\\00742.jpg\n",
      "./data/train\\00773.jpg\n",
      "./data/train\\00858.jpg\n",
      "./data/train\\00878.jpg\n",
      "./data/train\\00880.jpg\n",
      "./data/train\\00887.jpg\n",
      "./data/train\\00898.jpg\n",
      "./data/train\\00912.jpg\n",
      "./data/train\\00920.jpg\n",
      "./data/train\\00946.jpg\n",
      "./data/train\\01010.jpg\n",
      "./data/train\\01012.jpg\n",
      "./data/train\\01031.jpg\n",
      "./data/train\\01034.jpg\n",
      "./data/train\\01105.jpg\n",
      "./data/train\\01134.jpg\n",
      "./data/train\\01148.jpg\n",
      "./data/train\\01186.jpg\n",
      "./data/train\\01214.jpg\n",
      "./data/train\\01255.jpg\n",
      "./data/train\\01264.jpg\n",
      "./data/train\\01276.jpg\n",
      "./data/train\\01277.jpg\n",
      "./data/train\\01283.jpg\n",
      "./data/train\\01322.jpg\n",
      "./data/train\\01331.jpg\n",
      "./data/train\\01361.jpg\n",
      "./data/train\\01392.jpg\n",
      "./data/train\\01413.jpg\n",
      "./data/train\\01416.jpg\n",
      "./data/train\\01468.jpg\n",
      "./data/train\\01483.jpg\n",
      "./data/train\\01506.jpg\n",
      "./data/train\\01535.jpg\n",
      "./data/train\\01537.jpg\n",
      "./data/train\\01617.jpg\n",
      "./data/train\\01661.jpg\n",
      "./data/train\\01678.jpg\n",
      "./data/train\\01680.jpg\n",
      "./data/train\\01705.jpg\n",
      "./data/train\\01706.jpg\n",
      "./data/train\\01716.jpg\n",
      "./data/train\\01750.jpg\n",
      "./data/train\\01768.jpg\n",
      "./data/train\\01786.jpg\n",
      "./data/train\\01819.jpg\n",
      "./data/train\\01832.jpg\n",
      "./data/train\\01852.jpg\n",
      "./data/train\\01864.jpg\n",
      "./data/train\\01870.jpg\n",
      "./data/train\\01891.jpg\n",
      "./data/train\\01901.jpg\n",
      "./data/train\\01911.jpg\n",
      "./data/train\\01918.jpg\n",
      "./data/train\\01952.jpg\n",
      "./data/train\\01972.jpg\n",
      "./data/train\\02004.jpg\n",
      "./data/train\\02050.jpg\n",
      "./data/train\\02058.jpg\n",
      "./data/train\\02065.jpg\n",
      "./data/train\\02095.jpg\n",
      "./data/train\\02111.jpg\n",
      "./data/train\\02124.jpg\n",
      "./data/train\\02145.jpg\n",
      "./data/train\\02194.jpg\n",
      "./data/train\\02240.jpg\n",
      "./data/train\\02274.jpg\n",
      "./data/train\\02286.jpg\n",
      "./data/train\\02305.jpg\n",
      "./data/train\\02311.jpg\n",
      "./data/train\\02314.jpg\n",
      "./data/train\\02326.jpg\n",
      "./data/train\\02359.jpg\n",
      "./data/train\\02363.jpg\n",
      "./data/train\\02364.jpg\n",
      "./data/train\\02377.jpg\n",
      "./data/train\\02378.jpg\n",
      "./data/train\\02438.jpg\n",
      "./data/train\\02457.jpg\n",
      "./data/train\\02484.jpg\n",
      "./data/train\\02491.jpg\n",
      "./data/train\\02540.jpg\n",
      "./data/train\\02577.jpg\n",
      "./data/train\\02589.jpg\n",
      "./data/train\\02594.jpg\n",
      "./data/train\\02605.jpg\n",
      "./data/train\\02645.jpg\n",
      "./data/train\\02663.jpg\n",
      "./data/train\\02669.jpg\n",
      "./data/train\\02670.jpg\n",
      "./data/train\\02671.jpg\n",
      "./data/train\\02738.jpg\n",
      "./data/train\\02739.jpg\n",
      "./data/train\\02771.jpg\n",
      "./data/train\\02832.jpg\n",
      "./data/train\\02843.jpg\n",
      "./data/train\\02848.jpg\n",
      "./data/train\\02897.jpg\n",
      "./data/train\\02911.jpg\n",
      "./data/train\\02913.jpg\n",
      "./data/train\\02919.jpg\n",
      "./data/train\\02993.jpg\n",
      "./data/train\\03008.jpg\n",
      "./data/train\\03009.jpg\n",
      "./data/train\\03011.jpg\n",
      "./data/train\\03030.jpg\n",
      "./data/train\\03051.jpg\n",
      "./data/train\\03054.jpg\n",
      "./data/train\\03064.jpg\n",
      "./data/train\\03081.jpg\n",
      "./data/train\\03101.jpg\n",
      "./data/train\\03102.jpg\n",
      "./data/train\\03127.jpg\n",
      "./data/train\\03152.jpg\n",
      "./data/train\\03165.jpg\n",
      "./data/train\\03169.jpg\n",
      "./data/train\\03191.jpg\n",
      "./data/train\\03238.jpg\n",
      "./data/train\\03239.jpg\n",
      "./data/train\\03241.jpg\n",
      "./data/train\\03243.jpg\n",
      "./data/train\\03265.jpg\n",
      "./data/train\\03267.jpg\n",
      "./data/train\\03308.jpg\n",
      "./data/train\\03311.jpg\n",
      "./data/train\\03316.jpg\n",
      "./data/train\\03317.jpg\n",
      "./data/train\\03379.jpg\n",
      "./data/train\\03390.jpg\n",
      "./data/train\\03393.jpg\n",
      "./data/train\\03417.jpg\n",
      "./data/train\\03454.jpg\n",
      "./data/train\\03467.jpg\n",
      "./data/train\\03511.jpg\n",
      "./data/train\\03512.jpg\n",
      "./data/train\\03523.jpg\n",
      "./data/train\\03537.jpg\n",
      "./data/train\\03575.jpg\n",
      "./data/train\\03589.jpg\n",
      "./data/train\\03607.jpg\n",
      "./data/train\\03608.jpg\n",
      "./data/train\\03641.jpg\n",
      "./data/train\\03659.jpg\n",
      "./data/train\\03662.jpg\n",
      "./data/train\\03670.jpg\n",
      "./data/train\\03690.jpg\n",
      "./data/train\\03715.jpg\n",
      "./data/train\\03732.jpg\n",
      "./data/train\\03761.jpg\n",
      "./data/train\\03767.jpg\n",
      "./data/train\\03772.jpg\n",
      "./data/train\\03790.jpg\n",
      "./data/train\\03795.jpg\n",
      "./data/train\\03813.jpg\n",
      "./data/train\\03828.jpg\n",
      "./data/train\\03854.jpg\n",
      "./data/train\\03871.jpg\n",
      "./data/train\\03875.jpg\n",
      "./data/train\\03877.jpg\n",
      "./data/train\\03881.jpg\n",
      "./data/train\\03943.jpg\n",
      "./data/train\\03965.jpg\n",
      "./data/train\\03968.jpg\n",
      "./data/train\\04011.jpg\n",
      "./data/train\\04056.jpg\n",
      "./data/train\\04072.jpg\n",
      "./data/train\\04085.jpg\n",
      "./data/train\\04121.jpg\n",
      "./data/test\\04186.jpg\n",
      "./data/test\\04190.jpg\n",
      "./data/test\\04197.jpg\n",
      "./data/test\\04216.jpg\n",
      "./data/test\\04224.jpg\n",
      "./data/test\\04244.jpg\n",
      "./data/test\\04247.jpg\n",
      "./data/test\\04252.jpg\n",
      "./data/test\\04308.jpg\n",
      "./data/test\\04338.jpg\n",
      "./data/test\\04349.jpg\n",
      "./data/test\\04386.jpg\n",
      "./data/test\\04392.jpg\n",
      "./data/test\\04396.jpg\n",
      "./data/test\\04470.jpg\n",
      "./data/test\\04482.jpg\n",
      "./data/test\\04484.jpg\n",
      "./data/test\\04486.jpg\n",
      "./data/test\\04493.jpg\n",
      "./data/test\\04494.jpg\n",
      "./data/test\\04495.jpg\n",
      "./data/test\\04509.jpg\n",
      "./data/test\\04510.jpg\n",
      "./data/test\\04511.jpg\n",
      "./data/test\\04544.jpg\n",
      "./data/test\\04552.jpg\n",
      "./data/test\\04578.jpg\n",
      "./data/test\\04588.jpg\n",
      "./data/test\\04608.jpg\n",
      "./data/test\\04631.jpg\n",
      "./data/test\\04651.jpg\n",
      "./data/test\\04669.jpg\n",
      "./data/test\\04691.jpg\n",
      "./data/test\\04724.jpg\n",
      "./data/test\\04725.jpg\n",
      "./data/test\\04750.jpg\n",
      "./data/test\\04816.jpg\n",
      "./data/test\\04825.jpg\n",
      "./data/test\\04827.jpg\n",
      "./data/test\\04833.jpg\n",
      "./data/test\\04874.jpg\n",
      "./data/test\\04884.jpg\n",
      "./data/test\\04892.jpg\n",
      "./data/test\\04942.jpg\n",
      "./data/test\\05023.jpg\n",
      "./data/test\\05030.jpg\n",
      "./data/test\\05042.jpg\n",
      "./data/test\\05048.jpg\n",
      "./data/test\\05052.jpg\n",
      "./data/test\\05057.jpg\n",
      "./data/test\\05077.jpg\n",
      "./data/test\\05125.jpg\n",
      "./data/test\\05128.jpg\n",
      "./data/test\\05143.jpg\n",
      "./data/test\\05197.jpg\n",
      "./data/test\\05224.jpg\n",
      "./data/test\\05259.jpg\n",
      "./data/test\\05261.jpg\n",
      "./data/test\\05281.jpg\n",
      "./data/test\\05295.jpg\n",
      "./data/test\\05304.jpg\n",
      "./data/test\\05306.jpg\n",
      "./data/test\\05307.jpg\n",
      "./data/test\\05324.jpg\n",
      "./data/test\\05333.jpg\n",
      "./data/test\\05347.jpg\n",
      "./data/test\\05349.jpg\n",
      "./data/test\\05366.jpg\n",
      "./data/test\\05373.jpg\n",
      "./data/test\\05382.jpg\n",
      "./data/test\\05386.jpg\n",
      "./data/test\\05405.jpg\n",
      "./data/test\\05434.jpg\n",
      "./data/test\\05534.jpg\n",
      "./data/test\\05545.jpg\n",
      "./data/test\\05567.jpg\n",
      "./data/test\\05579.jpg\n",
      "./data/test\\05657.jpg\n",
      "./data/test\\05671.jpg\n",
      "./data/test\\05694.jpg\n",
      "./data/test\\05718.jpg\n",
      "./data/test\\05748.jpg\n",
      "./data/test\\05754.jpg\n",
      "./data/test\\05804.jpg\n",
      "./data/test\\05821.jpg\n",
      "./data/test\\05909.jpg\n",
      "./data/test\\05916.jpg\n",
      "./data/test\\05918.jpg\n",
      "./data/test\\05921.jpg\n",
      "./data/test\\05930.jpg\n",
      "./data/test\\05967.jpg\n",
      "./data/test\\05979.jpg\n",
      "./data/test\\06002.jpg\n",
      "./data/test\\06024.jpg\n",
      "./data/test\\06052.jpg\n",
      "./data/test\\06061.jpg\n",
      "./data/test\\06099.jpg\n",
      "./data/test\\06145.jpg\n",
      "./data/test\\06158.jpg\n",
      "./data/test\\06159.jpg\n",
      "./data/test\\06174.jpg\n",
      "./data/test\\06179.jpg\n",
      "./data/test\\06183.jpg\n",
      "./data/test\\06196.jpg\n",
      "./data/test\\06206.jpg\n",
      "./data/test\\06239.jpg\n",
      "./data/test\\06244.jpg\n",
      "./data/test\\06262.jpg\n",
      "./data/test\\06269.jpg\n",
      "./data/test\\06292.jpg\n",
      "./data/test\\06296.jpg\n",
      "./data/test\\06302.jpg\n",
      "./data/test\\06309.jpg\n",
      "./data/test\\06313.jpg\n",
      "./data/test\\06328.jpg\n",
      "./data/test\\06375.jpg\n",
      "./data/test\\06381.jpg\n",
      "./data/test\\06388.jpg\n",
      "./data/test\\06406.jpg\n",
      "./data/test\\06422.jpg\n",
      "./data/test\\06453.jpg\n",
      "./data/test\\06480.jpg\n",
      "./data/test\\06485.jpg\n",
      "./data/test\\06516.jpg\n",
      "./data/test\\06518.jpg\n",
      "./data/test\\06530.jpg\n",
      "./data/test\\06538.jpg\n",
      "./data/test\\06539.jpg\n",
      "./data/test\\06571.jpg\n",
      "./data/test\\06603.jpg\n",
      "./data/test\\06614.jpg\n",
      "./data/test\\06624.jpg\n",
      "./data/test\\06644.jpg\n",
      "./data/test\\06660.jpg\n",
      "./data/test\\06675.jpg\n",
      "./data/test\\06683.jpg\n",
      "./data/test\\06718.jpg\n",
      "./data/test\\06751.jpg\n",
      "./data/test\\06771.jpg\n",
      "./data/test\\06774.jpg\n",
      "./data/test\\06782.jpg\n",
      "./data/test\\06784.jpg\n",
      "./data/test\\06816.jpg\n",
      "./data/test\\06832.jpg\n",
      "./data/test\\06833.jpg\n",
      "./data/test\\06839.jpg\n",
      "./data/test\\06850.jpg\n",
      "./data/test\\06896.jpg\n",
      "./data/test\\06906.jpg\n",
      "./data/test\\06921.jpg\n",
      "./data/test\\06974.jpg\n",
      "./data/test\\07025.jpg\n",
      "./data/test\\07050.jpg\n",
      "./data/test\\07061.jpg\n",
      "./data/test\\07121.jpg\n",
      "./data/test\\07169.jpg\n",
      "./data/test\\07171.jpg\n",
      "./data/test\\07174.jpg\n",
      "./data/test\\07175.jpg\n",
      "./data/test\\07191.jpg\n",
      "./data/test\\07214.jpg\n",
      "./data/test\\07278.jpg\n",
      "./data/test\\07279.jpg\n",
      "./data/test\\07290.jpg\n",
      "./data/test\\07346.jpg\n",
      "./data/test\\07351.jpg\n",
      "./data/test\\07364.jpg\n",
      "./data/test\\07386.jpg\n",
      "./data/test\\07392.jpg\n",
      "./data/test\\07445.jpg\n",
      "./data/test\\07450.jpg\n",
      "./data/test\\07463.jpg\n",
      "./data/test\\07471.jpg\n",
      "./data/test\\07483.jpg\n",
      "./data/test\\07487.jpg\n",
      "./data/test\\07599.jpg\n",
      "./data/test\\07661.jpg\n",
      "./data/test\\07663.jpg\n",
      "./data/test\\07666.jpg\n",
      "./data/test\\07684.jpg\n",
      "./data/test\\07696.jpg\n",
      "./data/test\\07704.jpg\n",
      "./data/test\\07706.jpg\n",
      "./data/test\\07752.jpg\n",
      "./data/test\\07769.jpg\n",
      "./data/test\\07800.jpg\n",
      "./data/test\\07833.jpg\n",
      "./data/test\\07860.jpg\n",
      "./data/test\\07897.jpg\n",
      "./data/test\\07916.jpg\n",
      "./data/test\\07960.jpg\n",
      "./data/test\\07966.jpg\n",
      "./data/test\\08005.jpg\n",
      "./data/test\\08011.jpg\n",
      "./data/test\\08016.jpg\n",
      "./data/test\\08018.jpg\n",
      "./data/test\\08031.jpg\n",
      "./data/test\\08044.jpg\n",
      "./data/test\\08085.jpg\n",
      "./data/test\\08096.jpg\n",
      "./data/test\\08127.jpg\n",
      "./data/test\\08133.jpg\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# Функция обработки папки. Ожидается, что в этой папке лежит файл results.txt с метками классов, и \n",
    "# имеются подразделы train и test с jpg файлами.\n",
    "def process_folder(folder):\n",
    "    ydict = load_txt(folder + '/results.txt')\n",
    "    X, Y = get_features(folder + '/train/*jpg', ydict)\n",
    "    # Ваш код здесь. \n",
    "    X_test, Y_test = get_features(folder + '/test/*jpg', ydict)\n",
    "    \n",
    "    # Ваш код здесь. \n",
    "    clf = SVC(random_state=0)\n",
    "    clf.fit(X, Y)\n",
    "    \n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(sum(Y_test == Y_test_pred)) # Число правильно предсказанных классов\n",
    "    \n",
    "process_folder('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_answerNum(\"vgg16_answer3.txt\",89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}