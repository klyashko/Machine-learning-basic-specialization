{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from nltk import WordNetLemmatizer, word_tokenize, pos_tag\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization_data(data):\n",
    "    tokens_list = [word_tokenize(raw) for raw in data]\n",
    "    tokens_list = [[wnl.lemmatize(t, 'v') for t in tokens] for tokens in tokens_list]\n",
    "    \n",
    "    return [' '.join(x) for x in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission_fit(model, X, y):\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X, y, cv=kf)\n",
    "    \n",
    "    print(score)\n",
    "    print(score.mean())\n",
    "    print(score.std())\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    competition_data = pd.read_csv('data/products_sentiment_test.tsv', delimiter='\\t')\n",
    "    predictions = model.predict(lemmatization_data(competition_data.text))\n",
    "    \n",
    "    df = pd.DataFrame({'y': predictions})\n",
    "    df.index.name = 'Id'\n",
    "    df.to_csv('data/results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission(model):\n",
    "    competition_data = pd.read_csv('data/products_sentiment_test.tsv', delimiter='\\t')\n",
    "    predictions = model.predict(lemmatization_data(competition_data.text))\n",
    "    \n",
    "    df = pd.DataFrame({'y': predictions})\n",
    "    df.index.name = 'Id'\n",
    "    df.to_csv('data/results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_and_test_data():\n",
    "    train_data = pd.read_csv('data/products_sentiment_train.tsv', delimiter='\\t', header=None)\n",
    "    train_data.columns = ['text', 'class']\n",
    "\n",
    "    counts = train_data['class'].value_counts()\n",
    "\n",
    "    class_0 = int(counts[0] / 10)\n",
    "    class_1 = int(counts[1] / 10)\n",
    "\n",
    "    test_data = train_data[train_data['class'] == 0].sample(class_0)\n",
    "    test_data = test_data.append(train_data[train_data['class'] == 1].sample(class_1))\n",
    "\n",
    "    train_data = train_data.drop(test_data.index)\n",
    "\n",
    "    X = train_data.text.values\n",
    "    y = train_data['class'].values\n",
    "\n",
    "    X = lemmatization_data(X)\n",
    "\n",
    "    test_X = test_data.text.values\n",
    "    test_y = test_data['class'].values\n",
    "\n",
    "    test_X = lemmatization_data(test_X)\n",
    "    \n",
    "    return X, y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    train_data = pd.read_csv('data/products_sentiment_train.tsv', delimiter='\\t', header=None)\n",
    "    train_data.columns = ['text', 'class']\n",
    "\n",
    "    return lemmatization_data(train_data.text.values), train_data['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем протестировать модели из списка на базовый параметрах для выявления кондидатов на дальнейшее рассмотрение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier, RidgeClassifier, LinearSVC, SVC, MultinomialNB, KNeighborsClassifier, NearestCentroid, RandomForestClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790116601888\n",
      "\n",
      "Test score 0.824120603015\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "X, y, test_X, text_y = get_train_and_test_data()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 5), analyzer='char')), \n",
    "    ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "    ('classifier', LogisticRegression(C=10, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vectorizer__max_features': [5000, 10000, 15000, 20000, 25000]\n",
    "        'vectorizer__ngram_range': [(1, 2), (1, 3), (1, 4), (1, 5)],\n",
    "        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "        'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=kf, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print()\n",
    "for k in grid.best_params_:\n",
    "    print(k, \" : \", grid.best_params_[k])\n",
    "\n",
    "\n",
    "prediction = grid.best_estimator_.predict(test_X)\n",
    "\n",
    "print(\"Test score\", accuracy_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.79 on test\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "0.782343142699\n",
    "\n",
    "vectorizer__max_features  :  20000\n",
    "\n",
    "Test score 0.788944723618\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "0.786785119378\n",
    "\n",
    "classifier__C  :  10\n",
    "\n",
    "vectorizer__analyzer  :  char\n",
    "\n",
    "vectorizer__ngram_range  :  (1, 5)\n",
    "\n",
    "Test score 0.819095477387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.77290, std: 0.01281, params: {'vectorizer__max_features': 5000},\n",
       " mean: 0.77624, std: 0.01111, params: {'vectorizer__max_features': 10000},\n",
       " mean: 0.77957, std: 0.01156, params: {'vectorizer__max_features': 15000},\n",
       " mean: 0.78234, std: 0.01514, params: {'vectorizer__max_features': 20000},\n",
       " mean: 0.77901, std: 0.01444, params: {'vectorizer__max_features': 25000}]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772348695169\n",
      "\n",
      "Test score 0.809045226131\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "X, y, test_X, text_y = get_train_and_test_data()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 5), analyzer='char', max_features=20000)), \n",
    "    ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "    ('classifier', LinearSVC(C=1, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vectorizer__max_features': [5000, 10000, 15000, 20000, 25000]\n",
    "        'vectorizer__ngram_range': [(1, 2), (1, 3), (1, 4), (1, 5)],\n",
    "        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "        'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=kf, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print()\n",
    "for k in grid.best_params_:\n",
    "    print(k, \" : \", grid.best_params_[k])\n",
    "\n",
    "\n",
    "prediction = grid.best_estimator_.predict(test_X)\n",
    "\n",
    "print(\"Test score\", accuracy_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.79 on test\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "0.782343142699\n",
    "\n",
    "vectorizer__max_features  :  20000\n",
    "\n",
    "Test score 0.78391959799\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "0.789006107718\n",
    "\n",
    "classifier__C  :  1\n",
    "\n",
    "vectorizer__analyzer  :  char\n",
    "\n",
    "vectorizer__ngram_range  :  (1, 5)\n",
    "\n",
    "Test score 0.748743718593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "\n",
    "Лучший счет на нестовой выборке 0.815 что является мягко говоря не очень хорошим результатом.\n",
    "Модель явно тербует доработки. Так же возможно что исходные данные требуют более детельного рассмотрения.\n",
    "Но на данный момент принято решение отложить этот вопрос."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
